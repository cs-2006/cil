{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for training and testing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from typing import Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# For this approach we use a pre-trained model which is based on RoBERTa\n",
    "model_name = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "output_dir = \"./workspace/data/\"\n",
    "\n",
    "# load the preprocessed data\n",
    "data = pd.read_csv(\"data_cleaned.csv\")\n",
    "data_sample = data.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "if model_name.__contains__(\"cardiffnlp\"):\n",
    "    data_sample[\"text\"] = data_sample[\"text\"].apply(lambda x: x.replace(\"@Alex\", \"@user\").replace(\"@Sam\", \"@user\").replace(\"@Taylor\", \"@user\").replace(\"<url>\", \"http\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078e097703d940bc89b2e8cb6ce766d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad5e7af29a94ae48d44cefe21ecdda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a73016d520740b2ae5dcd31e58d9b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4ddb4e4b8a432693a242ca61ce3794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5221ca8633184dd69e572e54d01ee469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:124647939\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model as well as the tokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings\n",
    "print(\"model parameters:\" + str(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the labels into one-hot encoding\n",
    "\n",
    "\n",
    "labels = data_sample[\"label\"].tolist()\n",
    "labels = [0 if x == 0 else 2 for x in labels]\n",
    "labels = np.eye(3)[labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1401857c8b4d4d8d3867a4ae26567c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2261322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99627599f6184c11b669deed559472fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2261322\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the training and validation sets\n",
    "\n",
    "train_data,val_data, train_labels, val_labels = train_test_split(data_sample[\"text\"], labels, test_size=5000/len(data_sample), random_state=42)\n",
    "dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(train_data, train_labels)])\n",
    "val_dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(val_data, val_labels)])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240705_073701-qpy0il9n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ethz1/huggingface/runs/qpy0il9n' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/ethz1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ethz1/huggingface' target=\"_blank\">https://wandb.ai/ethz1/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ethz1/huggingface/runs/qpy0il9n' target=\"_blank\">https://wandb.ai/ethz1/huggingface/runs/qpy0il9n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8834' max='8834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8834/8834 33:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.214203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.201095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.188866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.190119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.185359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.176505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.180291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.182714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>0.176638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.181211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.172632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.173684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.170109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>0.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>0.170290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.175126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.164201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.165587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>0.166265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.161843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.163716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.164176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.165643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.162649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.164046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.159752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.160254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.160536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.157150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.158999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.154961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.156082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.154518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.153334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.156700</td>\n",
       "      <td>0.153514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.152327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.152780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.152611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.179700</td>\n",
       "      <td>0.152299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.152234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.152157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.152203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.152219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8616\n",
      "Evaluation Accuracy: 0.8714\n",
      "Evaluation Accuracy: 0.8734\n",
      "Evaluation Accuracy: 0.8764\n",
      "Evaluation Accuracy: 0.8718\n",
      "Evaluation Accuracy: 0.8842\n",
      "Evaluation Accuracy: 0.8870\n",
      "Evaluation Accuracy: 0.8802\n",
      "Evaluation Accuracy: 0.8644\n",
      "Evaluation Accuracy: 0.8868\n",
      "Evaluation Accuracy: 0.8638\n",
      "Evaluation Accuracy: 0.8812\n",
      "Evaluation Accuracy: 0.8862\n",
      "Evaluation Accuracy: 0.8890\n",
      "Evaluation Accuracy: 0.8846\n",
      "Evaluation Accuracy: 0.8696\n",
      "Evaluation Accuracy: 0.8574\n",
      "Evaluation Accuracy: 0.8868\n",
      "Evaluation Accuracy: 0.8880\n",
      "Evaluation Accuracy: 0.8846\n",
      "Evaluation Accuracy: 0.8946\n",
      "Evaluation Accuracy: 0.8826\n",
      "Evaluation Accuracy: 0.8636\n",
      "Evaluation Accuracy: 0.8932\n",
      "Evaluation Accuracy: 0.8920\n",
      "Evaluation Accuracy: 0.8894\n",
      "Evaluation Accuracy: 0.8898\n",
      "Evaluation Accuracy: 0.8778\n",
      "Evaluation Accuracy: 0.8968\n",
      "Evaluation Accuracy: 0.8962\n",
      "Evaluation Accuracy: 0.8934\n",
      "Evaluation Accuracy: 0.8940\n",
      "Evaluation Accuracy: 0.8978\n",
      "Evaluation Accuracy: 0.8966\n",
      "Evaluation Accuracy: 0.8940\n",
      "Evaluation Accuracy: 0.9002\n",
      "Evaluation Accuracy: 0.8934\n",
      "Evaluation Accuracy: 0.8958\n",
      "Evaluation Accuracy: 0.8968\n",
      "Evaluation Accuracy: 0.8990\n",
      "Evaluation Accuracy: 0.8994\n",
      "Evaluation Accuracy: 0.8984\n",
      "Evaluation Accuracy: 0.8988\n",
      "Evaluation Accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "# Transfer learning part - define the training arguments and the trainer\n",
    "\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl,training_args\n",
    "\n",
    "# custom callback class for accuracy evaluation during the training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Assuming the evaluation dataset has 'labels' and 'predictions' fields\n",
    "        eval_dataloader = kwargs['eval_dataloader']\n",
    "        model = kwargs['model']\n",
    "        tokenizer = kwargs['tokenizer']\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in eval_dataloader:\n",
    "            inputs = batch['input_ids'].to(args.device)\n",
    "            labels = batch['labels'].to(args.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            labels = torch.argmax(labels, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# initialization of the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=256,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=training_args.OptimizerNames.ADAMW_TORCH, learning_rate=1e-4,\n",
    "        # optim=training_args.OptimizerNames.LION,learning_rate=1e-5,\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        num_train_epochs=1,\n",
    "        report_to=\"wandb\",\n",
    "        # report_to=\"none\",\n",
    "        group_by_length=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "    ),\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model and the tokenizer\n",
    "\n",
    "torch.save(model, \"bert.pt\")\n",
    "torch.save(tokenizer, \"bert_tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the submission test data and save the predictions\n",
    "\n",
    "from tqdm import tqdm\n",
    "test_df = pd.read_csv(\"test_data_cleaned.csv\")\n",
    "model = model.cuda()\n",
    "model = model.eval()\n",
    "\n",
    "batch_size=50\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_df), batch_size)):\n",
    "        batch = test_df[\"text\"][i:i+batch_size].tolist()\n",
    "        tokens = tokenizer(batch, padding=True, return_tensors=\"pt\")\n",
    "        tokens = {k: v.cuda() for k, v in tokens.items()}\n",
    "        output = model(**tokens)\n",
    "        logits = output[0].cpu()\n",
    "        scores = F.softmax(logits, dim=1)[:,2] # 0 -> Negative; 1 -> Neutral; 2 -> Positive\n",
    "        all_preds.extend(scores.tolist())\n",
    "\n",
    "binary_predictions = [1 if prob >= 0.5 else -1 for prob in all_preds]\n",
    "submission_df = pd.DataFrame({\"Prediction\": binary_predictions})\n",
    "submission_df[\"Id\"] = submission_df.index + 1\n",
    "submission_df = submission_df[[\"Id\", \"Prediction\"]]\n",
    "submission_df.to_csv(\"submission_bert.csv\", index=False)\n",
    "\n",
    "test_probs_df = pd.DataFrame({'Probability': all_preds})\n",
    "test_probs_df.to_csv(\"test_probs_bert.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1be15a159d9874788f7b7854451912393d9e82d0d2bc47d83a870bda7fd9bc22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
